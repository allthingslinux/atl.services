// Logging: Collect Docker container logs
loki.source.docker "containers" {
  host       = "unix:///var/run/docker.sock"
  targets    = []
  forward_to = [loki.process.scrub_logs.receiver]
  
  relabel_rules {
    source_labels = ["__meta_docker_container_name"]
    target_label  = "container"
  }
}

// Log Scrubbing: Mask sensitive data
loki.process "scrub_logs" {
  forward_to = [loki.write.default.receiver]
  
  stage.replace {
    expression = "(\\d{1,3}\\.){3}\\d{1,3}"
    replace = "***IP***"
  }
  stage.replace {
    expression = "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"
    replace = "***EMAIL***"
  }
}

// Logging: Forward to Loki
loki.write "default" {
  endpoint {
    url = "http://loki:3100/loki/api/v1/push"
  }
}

// Tracing: Receive OTLP traces and forward to Tempo (if enabled) or discard/log 
// Note: Tempo was removed, so we point to nowhere or maybe just log for now?
// Actually the previous config pointed to 100.64.2.0:4317 which is self.
// Since Tempo is removed from the stack, we should probably remove this or point to DevNull 
// But let's leave OTLP receiver for future use, just dropping output or logging.
// Wait, user might want to add Tempo later. For now let's keep it minimal or remove trace section if unused.
// Previous plan had "Forward to Tempo". Since Tempo is removed, I will comment this out or remove.

// Metrics: Forward to Mimir
prometheus.remote_write "default" {
  endpoint {
    url = "http://mimir:8080/api/v1/push"
  }
}

// ============================================
// Scrape Jobs (Migrated from prometheus.yml)
// ============================================

// Local System
prometheus.scrape "node_exporter" {
  targets = [{"__address__" = "node-exporter:9100"}]
  job_name = "node-exporter"
  forward_to = [prometheus.remote_write.default.receiver]
}

prometheus.scrape "cadvisor" {
  targets = [{"__address__" = "cadvisor:8080"}]
  job_name = "cadvisor"
  forward_to = [prometheus.remote_write.default.receiver]
}

// Service Exporters
prometheus.scrape "service_exporters" {
  targets = [
    {"__address__" = "postgres-exporter-xmpp:9187", "job" = "postgres-xmpp"},
    {"__address__" = "postgres-exporter-tux:9187", "job" = "postgres-tux"},
    {"__address__" = "postgres-exporter-iso:9187", "job" = "postgres-iso"},
    {"__address__" = "redis-exporter-tux:9121", "job" = "valkey-tux"},
    {"__address__" = "redis-exporter-wiki:9121", "job" = "valkey-wiki"},
    {"__address__" = "mysql-exporter-wiki:9104", "job" = "mariadb-wiki"},
    {"__address__" = "nginx-exporter-wiki:9113", "job" = "nginx-wiki"},
    {"__address__" = "cloudflare-exporter:9199", "job" = "cloudflare-exporter"},
  ]
  forward_to = [prometheus.remote_write.default.receiver]
}

// Local Services
prometheus.scrape "local_services" {
  targets = [
    {"__address__" = "gatus:8080", "job" = "gatus"},
    {"__address__" = "blocky:4000", "job" = "blocky"},
    {"__address__" = "sftpgo:10000", "job" = "sftpgo"},
    {"__address__" = "influxdb2:8086", "job" = "influxdb2"},
  ]
  forward_to = [prometheus.remote_write.default.receiver]
}

// Metrics from Chat Stack (Prosody & IRC)
// Note: These targets are reachable via network alias or IP in the same compose network if they are in the same network.
// If they are on other VPSs, we rely on Alloy on THOSE VPSs to push to Mimir.
// BUT: The user's `prometheus.yml` had targets like `atl-xmpp-server:5280`. 
// If these are running in THIS stack's network (e.g. atl-chat network is attached), we can scrape them.
// compose.yaml has `atl-chat`, `tux`, `wiki-network` attached. So we can scrape them!

prometheus.scrape "prosody" {
  targets = [{"__address__" = "atl-xmpp-server:5280"}]
  metrics_path = "/metrics"
  job_name = "prosody"
  forward_to = [prometheus.remote_write.default.receiver]
}

prometheus.scrape "unrealircd" {
  targets = [{"__address__" = "atl-irc-server:8600"}]
  job_name = "unrealircd"
  forward_to = [prometheus.remote_write.default.receiver]
}

prometheus.scrape "keycloak" {
  targets = [{"__address__" = "iso_keycloak:8080"}]
  metrics_path = "/metrics"
  job_name = "keycloak"
  forward_to = [prometheus.remote_write.default.receiver]
}

prometheus.scrape "minio_wiki" {
  targets = [{"__address__" = "local-wiki-minio:9000"}]
  metrics_path = "/minio/v2/metrics/cluster"
  job_name = "minio-wiki"
  forward_to = [prometheus.remote_write.default.receiver]
}

// Blackbox Exporter - Monitor Endpoints
// We need to define the targets and relabel them to use the blackbox exporter

prometheus.scrape "blackbox" {
  targets = [
    {"__address__" = "https://metrics.atl.services", "module" = "http_2xx"},
    {"__address__" = "https://chat.atl.services",    "module" = "http_2xx"},
    {"__address__" = "https://wiki.atl.services",    "module" = "http_2xx"},
    {"__address__" = "https://iso.atl.dev",          "module" = "http_2xx"},
  ]
  metrics_path = "/probe"
  job_name = "blackbox"
  
  // Relabeling to pass target param
  relabel_rules {
    source_labels = ["__address__"]
    target_label  = "__param_target"
  }
  relabel_rules {
    source_labels = ["__param_target"]
    target_label  = "instance"
  }
  relabel_rules {
    source_labels = ["module"]
    target_label  = "__param_module"
  }
  relabel_rules {
    target_label = "__address__"
    replacement  = "blackbox:9115"
  }
  
  forward_to = [prometheus.remote_write.default.receiver]
}

prometheus.scrape "blackbox_exporter" {
  targets = [{"__address__" = "blackbox:9115"}]
  job_name = "blackbox-exporter"
  forward_to = [prometheus.remote_write.default.receiver]
}
